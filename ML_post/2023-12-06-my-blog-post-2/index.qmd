---
title: "Clustering"
description: "ML assignment blog 2"
date: 2023-12-02
categories: [Machine Learning] 
draft: false
---

K-means clustering is a popular unsupervised machine learning algorithm designed for partitioning a dataset into distinct groups, or clusters, based on similarity patterns among data points. The algorithm iteratively assigns each data point to the cluster whose mean is closest, forming clusters with minimized intra-cluster distances. K-means requires a pre-specified number of clusters, denoted by 'k.' The process continues until convergence, where the assignment of data points and the cluster centroids stabilize. The algorithm is efficient and widely used for tasks such as customer segmentation, image compression, and pattern recognition. However, its performance can be sensitive to the initial placement of cluster centroids, and the choice of 'k' requires careful consideration. Despite its limitations, K-means clustering serves as a foundational technique in exploratory data analysis and unsupervised learning.

## 1. Data

For clustering using K-means, we use the freely available USArrests dataset. We aim to cluster the states of US based on the crimes from the USAressts data.

```{r}

library(skimr)

data("USArrests")


skim(USArrests)

head(USArrests)



```

## 2. Identify and remove variables with high correlation

```{r}

library(GGally)


ggpairs(
  
  data = USArrests,
  columns = c(1:4))
  
```

From the analysis, we see that there's a high correlation between murder and assault. So, we have to remove either one of them. Here, we remove murder for the further clustering analysis.

## 3. Scale the variables

```{r}



#Scale

data_scaled = scale(USArrests)

head(data_scaled)

data_selected = data_scaled[, 2:4]

head(data_selected)
```

## 4. Required packages

```{r}

library(cluster)
library(factoextra)
library(ggplot2)


set.seed(123)
crime = sample(1:50, 10)

crime_1 = data_selected[crime,]

head(crime_1)

```

## 5. Euclidean Distance

```{r}

dist_eucli = dist(crime_1, method = "euclidean")

head(dist_eucli)

```

```{r}


round(as.matrix(dist_eucli)[1:4, 1:4], 1)

fviz_dist(dist_eucli)
```

## 6. Optimal number of clusters

```{r}


# Elbow method
fviz_nbclust(data_selected, kmeans, method = 'wss') +
             geom_vline(xintercept = 4, linetype=5, col= "green")+
    labs(subtitle = "Elbow method")


# Silhouette method
fviz_nbclust(data_selected, kmeans, method = "silhouette")+
  labs(subtitle = "Silhouette method")


```

```{r}


kmeans_1 = kmeans(data_selected, 4, nstart = 20)
kmeans_1

kmeans_1$betweenss

```

```{r}

df = cbind(data_selected, cluster = kmeans_1$cluster)

head(df)
```

```{r}


kmeans_2 = kmeans(data_selected, 5, nstart = 20)
kmeans_2

```

```{r}

kmeans_3 = kmeans(data_scaled, 4, nstart = 20)
kmeans_3

```

## 7. Visualizing clusters with 4 and 5 clusters

We use two different dataframes, data_selected and data_scaled for our clustering with 4 and 5 clusters. The variables without removing murder variable (data_scaled) for high correlation with assault variable has a better cluster visualization. In this case, using PCA would be really useful in creating new variable with less correlation without losing most information.

```{r}


fviz_cluster(kmeans_1, data = data_selected,
             palette=c("red", "blue", "black", "darkgreen"),
             ellipse.type = "euclid",
             star.plot = T,
             repel = T,
             ggtheme = theme())


fviz_cluster(kmeans_2, data = data_selected,
             palette=c("red", "blue", "black", "darkgreen", "yellow"),
             ellipse.type = "euclid",
             star.plot = T,
             repel = T,
             ggtheme = theme())


fviz_cluster(kmeans_3, data = data_scaled,
             palette=c("red", "blue", "black", "darkgreen"),
             ellipse.type = "euclid",
             star.plot = T,
             repel = T,
             ggtheme = theme())


```
